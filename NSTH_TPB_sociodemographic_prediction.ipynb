{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzd4oMX7t7Ua"
      },
      "source": [
        "# Downloading, and creating a dataframe from NHTS data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import wget\n",
        "import ssl\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor, CatBoostClassifier, Pool\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To address verfication error while dowloading the data\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
        "    pass\n",
        "else:\n",
        "    # Handle target environment that doesn't support HTTPS verification\n",
        "    ssl._create_default_https_context = _create_unverified_https_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nck4GOVByR7K",
        "outputId": "9c42aae2-72f4-43ce-ebdf-7cc64b77e0a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "if os.path.exists('./data/Ascii.zip'):\n",
        "     pass\n",
        "else:\n",
        "     if os.path.exists('./data/'):\n",
        "          pass\n",
        "     else:\n",
        "          os.mkdir('./data/')\n",
        "     # Download the survey data\n",
        "     url = 'https://nhts.ornl.gov/2009/download/Ascii.zip'\n",
        "     wget.download(url, './data/Ascii.zip')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQTclZ6CPijL",
        "outputId": "98f0a532-e775-4a97-eff7-a04e621376b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['DAYV2PUB.CSV', 'HHV2PUB.CSV', 'PERV2PUB.CSV', 'VEHV2PUB.CSV']\n"
          ]
        }
      ],
      "source": [
        "with zipfile.ZipFile(\"./data/Ascii.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"./data/\")\n",
        "    \n",
        "Dir = os.path.join('./data/Ascii')\n",
        "showlist = os.listdir(Dir)\n",
        "\n",
        "# Observe the folder's content\n",
        "print(showlist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0oU4Sx6Nt7R"
      },
      "source": [
        "After the zip file is downloaded, we'll unzip and extract its content into our directory.The unpacked file is consist of four .csv files as follows:\n",
        "\n",
        "- **Travel day trips**\n",
        "- Household Data \n",
        "- **Person Data**\n",
        "- Vehicle Data \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RMEL8gx4-W-n"
      },
      "outputs": [],
      "source": [
        "trp_dir = ('./data/Ascii/DAYV2PUB.CSV')\n",
        "prsn_dir = ('./data/Ascii/PERV2PUB.CSV')\n",
        "\n",
        "# df is Travel day trip dataframe\n",
        "df = pd.read_csv(trp_dir)\n",
        "\n",
        "# df_p is each person's dataframe\n",
        "df_p = pd.read_csv(prsn_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-udbo_sM0CR"
      },
      "source": [
        "# Preprocess; Reshaping the data as we need it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## National Houshold Travel Survey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q6uiAqZ5otyV"
      },
      "outputs": [],
      "source": [
        "# the lists of variables we wish to keep\n",
        "df = df[\n",
        "    ['HOUSEID', 'PERSONID','WHYFROM', 'WHYTO', 'TRPMILES',\n",
        "     'HHSTATE', 'STRTTIME','ENDTIME','TDTRPNUM', 'DWELTIME',\n",
        "     'URBRUR', 'TRVL_MIN','HBHUR','TRIPPURP', 'R_SEX', 'R_AGE',\n",
        "     'HHFAMINC', 'TRPTRANS'\n",
        "    ]\n",
        "]\n",
        "df_p = df_p[\n",
        "    ['HOUSEID', 'PERSONID', 'R_SEX', 'R_AGE', 'HHFAMINC','HHSIZE',\n",
        "     'HHVEHCNT', 'HHSTATE', 'URBRUR', 'SAMEPLC', 'HBHUR', 'TRAVDAY',\n",
        "     'HH_RACE', 'DRIVER', 'EDUC', 'OCCAT', 'SELF_EMP', 'WKFTPT', 'FRSTHM'\n",
        "    ]\n",
        "]\n",
        "\n",
        "# -1 are not nan values in this column.                   \n",
        "df_p['SAMEPLC'].replace([-1, -7], 0, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biBlfpirJZlo"
      },
      "source": [
        "## Replacing values in columns "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZUxQHLax6ld",
        "outputId": "d2fb2793-b214-4874-da76-dd758731e064"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.0    1021207\n",
              "4.0     100405\n",
              "3.0      26727\n",
              "5.0       9443\n",
              "6.0       7415\n",
              "Name: TRPTRANS, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Compacting trip modes to 5 main categories\n",
        "df['TRPTRANS'].replace([1, 2, 3, 4, 5, 6, 7, 8], 2, inplace=True) # auto\n",
        "df['TRPTRANS'].replace([9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 3, inplace=True) # transit\n",
        "df['TRPTRANS'].replace([23], 4, inplace=True) # walk\n",
        "df['TRPTRANS'].replace([22], 5, inplace=True) # bike\n",
        "df['TRPTRANS'].replace([97, 20, 21, 24], 6, inplace=True) # other\n",
        "\n",
        "df['TRPTRANS'].replace([-1, -7, -8, -9], np.nan, inplace=True)\n",
        "df['TRPTRANS'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCyOLXi8CaKJ"
      },
      "source": [
        "### Three new columns are defined to indicate the number of trips specifically seperated by purpose:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trips in the NHTS data are divided into detailed trip purposes. For the purpose of the paper, all these categories will be grouped into 3 main purposes: 1-work and education trips 2- shop trips 3- other trips "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcSN12uqRJbf",
        "outputId": "2b98e27c-b807-4098-8ced-cf2ef5c21aa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total value counts for education-work(mandatory) trips is: 92152\n",
            "Total value counts for shopping trips is 209607\n",
            "Total value counts for other trips is: 859952\n"
          ]
        }
      ],
      "source": [
        "# Change the items in the given list with\n",
        "work_education = [10, 11, 12] # = 2\n",
        "shop_trips = [40, 41, 42] # = 3\n",
        "other_trips = [20, 21, 22, 23, 24, 30, 43, 60,\n",
        "               61, 62, 63, 64, 65, 97, 80, 81,\n",
        "               82, 83, 70, 71, 72, 73, 50, 51,\n",
        "               52, 53, 54, 55, 1, 13, 14] # = 4\n",
        "\n",
        "# Regrouping purposes\n",
        "df['WHYTO'].replace(work_education, 2, inplace=True)\n",
        "df['WHYTO'].replace(shop_trips, 3, inplace=True)\n",
        "df['WHYTO'].replace(other_trips, 4, inplace=True)\n",
        "\n",
        "# Replacing not available data with NAN\n",
        "df['WHYTO'].replace([-1, -7, -8, -9], np.nan, inplace=True)\n",
        "df['STRTTIME'].replace([-1, -7, -8, -9], np.nan, inplace=True)\n",
        "df['TRIPPURP'].replace('-9', np.nan, inplace=True)\n",
        "\n",
        "\n",
        "#changing not available data for these two to 0 because we don't want to delete them\n",
        "df['DWELTIME'].replace([-1, -7, -8, -9], 0, inplace=True)\n",
        "df['TRPMILES'].replace([-1, -7, -8, -9], 0, inplace=True)\n",
        "\n",
        "value_counts = df['WHYTO'].value_counts().values\n",
        "print('Total value counts for education-work(mandatory) trips is: {}'.format(value_counts[2]))\n",
        "print('Total value counts for shopping trips is {}'.format(value_counts[1]))\n",
        "print('Total value counts for other trips is: {}'.format(value_counts[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2BYqHHgSp9Jx"
      },
      "outputs": [],
      "source": [
        "# defining 3 columns that shows the number of trips by category\n",
        "counts = df.groupby([\"HOUSEID\", \"PERSONID\", \"WHYTO\"]).apply(len).unstack(fill_value=0)\n",
        "counts.columns = counts.columns.map(lambda x: f\"WHYTO{x}\")\n",
        "df_p = df_p.merge(counts, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'WHYTO2.0': 'WORK_EDUCATION', 'WHYTO3.0': 'SHOP', 'WHYTO4.0': 'OTHER'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Home based and non-homebased trips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Home based trips in the data, are categorized as homebased shop, homebased work, homebased social-recreational, homebased other and non-homebased. I added the homebased social-recreational to homebased other to shrink the range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['HBO', 'NHB', 'HBSOCREC', 'HBSHOP', 'HBW', nan], dtype=object)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['TRIPPURP'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HDylkIpVYe2O"
      },
      "outputs": [],
      "source": [
        "# Recreational trips are in the other category\n",
        "df['TRIPPURP'].replace('HBSOCREC', 'HBO', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "89yDQO6CKHs3"
      },
      "outputs": [],
      "source": [
        "# Changing gender classification categories to 0(male) and 1(female)\n",
        "df_p['R_SEX'].replace(1, 0, inplace=True)\n",
        "df_p['R_SEX'].replace(2, 1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining time intervals to temporally count the trips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4 periods of day are defined to see how many trips are made in each period. These periods are each a column that will be added to the features later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OUKe5LLkKOf1"
      },
      "outputs": [],
      "source": [
        "# 1 = A.M\n",
        "# 2 = P.M\n",
        "# 3 = mid_day\n",
        "# 4 = night\n",
        "\n",
        "df['TRVL_DAY_TIME'] = np.where((df['STRTTIME'] >= 500) & (df['STRTTIME'] <= 1000), 1, \n",
        "                  np.where((df['STRTTIME'] >= 1001) & (df['STRTTIME'] <= 1530), 2,\n",
        "                  np.where((df['STRTTIME'] >= 1531) & (df['STRTTIME'] <= 2000), 3, 4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nPOkrJIknQDz"
      },
      "outputs": [],
      "source": [
        "# making the columns for each daytime and merging it to the person dataframe\n",
        "counts1 = df.groupby([\"HOUSEID\", \"PERSONID\", \"TRVL_DAY_TIME\"]).apply(len).unstack(fill_value=0)\n",
        "counts1.columns = counts1.columns.map(lambda x: f\"STRTTIME{x}\")\n",
        "df_p = df_p.merge(counts1, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'STRTTIME1': 'AM', 'STRTTIME2': 'PM', 'STRTTIME3': 'MIDDAY', 'STRTTIME4': 'NIGHT'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data was collected on a travel day. travel day is either a day on weekday or the weekend. To be able to process this later and analyse the difference, lets do the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "x5dqjF42K1a3"
      },
      "outputs": [],
      "source": [
        "# the data was collected based on trips on weekday or weekend?\n",
        "df_p['TRAVDAY'].replace([1, 7], 'weekend', inplace=True)\n",
        "df_p['TRAVDAY'].replace([2, 3, 4, 5, 6], 'weekday', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Time of the first and the last trip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N5eFGLJ2MTV1"
      },
      "outputs": [],
      "source": [
        "# This two new columns will show the time the person did their first and last trips as indicated in the name\n",
        "res =(df.sort_values([\"HOUSEID\",\"PERSONID\",\"TDTRPNUM\"])\n",
        "    .groupby([\"HOUSEID\", \"PERSONID\"], as_index=False)\n",
        "    .agg({\"STRTTIME\":\"first\",\"ENDTIME\":\"last\"})\n",
        "    .rename(columns={\"STRTTIME\":\"firsttrip_time\",\"ENDTIME\":\"lasttrip_time\"}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zcVkDzaEOhTS"
      },
      "outputs": [],
      "source": [
        "# the time is stored in military time format.\n",
        "# this will convert the number to integer numbers in range of 0 to 23\n",
        "res['firsttrip_time'] = np.where((res['firsttrip_time'] >= 0) & (res['firsttrip_time'] <= 100), 0, \n",
        "                  np.where((res['firsttrip_time'] >= 101) & (res['firsttrip_time'] <= 200), 1,\n",
        "                  np.where((res['firsttrip_time'] >= 201) & (res['firsttrip_time'] <= 300), 2,\n",
        "                  np.where((res['firsttrip_time'] >= 301) & (res['firsttrip_time'] <= 400), 3,\n",
        "                  np.where((res['firsttrip_time'] >= 401) & (res['firsttrip_time'] <= 500), 4,\n",
        "                  np.where((res['firsttrip_time'] >= 501) & (res['firsttrip_time'] <= 600), 5,\n",
        "                  np.where((res['firsttrip_time'] >= 601) & (res['firsttrip_time'] <= 700), 6,\n",
        "                  np.where((res['firsttrip_time'] >= 701) & (res['firsttrip_time'] <= 800), 7,\n",
        "                  np.where((res['firsttrip_time'] >= 801) & (res['firsttrip_time'] <= 900), 8,\n",
        "                  np.where((res['firsttrip_time'] >= 901) & (res['firsttrip_time'] <= 1000), 9,\n",
        "                  np.where((res['firsttrip_time'] >= 1001) & (res['firsttrip_time'] <= 1100), 10,\n",
        "                  np.where((res['firsttrip_time'] >= 1101) & (res['firsttrip_time'] <= 1200), 11,\n",
        "                  np.where((res['firsttrip_time'] >= 1201) & (res['firsttrip_time'] <= 1300), 12, \n",
        "                  np.where((res['firsttrip_time'] >= 1301) & (res['firsttrip_time'] <= 1400), 13,\n",
        "                  np.where((res['firsttrip_time'] >= 1401) & (res['firsttrip_time'] <= 1500), 14,\n",
        "                  np.where((res['firsttrip_time'] >= 1501) & (res['firsttrip_time'] <= 1600), 15,\n",
        "                  np.where((res['firsttrip_time'] >= 1601) & (res['firsttrip_time'] <= 1700), 16,\n",
        "                  np.where((res['firsttrip_time'] >= 1701) & (res['firsttrip_time'] <= 1800), 17,\n",
        "                  np.where((res['firsttrip_time'] >= 1801) & (res['firsttrip_time'] <= 1900), 18,\n",
        "                  np.where((res['firsttrip_time'] >= 1901) & (res['firsttrip_time'] <= 2000), 19,\n",
        "                  np.where((res['firsttrip_time'] >= 2001) & (res['firsttrip_time'] <= 2100), 20,\n",
        "                  np.where((res['firsttrip_time'] >= 2101) & (res['firsttrip_time'] <= 2200), 21,\n",
        "                  np.where((res['firsttrip_time'] >= 2201) & (res['firsttrip_time'] <= 2300), 22,\n",
        "                  np.where((res['firsttrip_time'] >= 2301) & (res['firsttrip_time'] <= 2359), 23,-9))))))))))))))))))))))))\n",
        "\n",
        "res['lasttrip_time'] = np.where((res['lasttrip_time'] >= 0) & (res['lasttrip_time'] <= 100), 0, \n",
        "                  np.where((res['lasttrip_time'] >= 101) & (res['lasttrip_time'] <= 200), 1,\n",
        "                  np.where((res['lasttrip_time'] >= 201) & (res['lasttrip_time'] <= 300), 2,\n",
        "                  np.where((res['lasttrip_time'] >= 301) & (res['lasttrip_time'] <= 400), 3,\n",
        "                  np.where((res['lasttrip_time'] >= 401) & (res['lasttrip_time'] <= 500), 4,\n",
        "                  np.where((res['lasttrip_time'] >= 501) & (res['lasttrip_time'] <= 600), 5,\n",
        "                  np.where((res['lasttrip_time'] >= 601) & (res['lasttrip_time'] <= 700), 6,\n",
        "                  np.where((res['lasttrip_time'] >= 701) & (res['lasttrip_time'] <= 800), 7,\n",
        "                  np.where((res['lasttrip_time'] >= 801) & (res['lasttrip_time'] <= 900), 8,\n",
        "                  np.where((res['lasttrip_time'] >= 901) & (res['lasttrip_time'] <= 1000), 9,\n",
        "                  np.where((res['lasttrip_time'] >= 1001) & (res['lasttrip_time'] <= 1100), 10,\n",
        "                  np.where((res['lasttrip_time'] >= 1101) & (res['lasttrip_time'] <= 1200), 11,\n",
        "                  np.where((res['lasttrip_time'] >= 1201) & (res['lasttrip_time'] <= 1300), 12, \n",
        "                  np.where((res['lasttrip_time'] >= 1301) & (res['lasttrip_time'] <= 1400), 13,\n",
        "                  np.where((res['lasttrip_time'] >= 1401) & (res['lasttrip_time'] <= 1500), 14,\n",
        "                  np.where((res['lasttrip_time'] >= 1501) & (res['lasttrip_time'] <= 1600), 15,\n",
        "                  np.where((res['lasttrip_time'] >= 1601) & (res['lasttrip_time'] <= 1700), 16,\n",
        "                  np.where((res['lasttrip_time'] >= 1701) & (res['lasttrip_time'] <= 1800), 17,\n",
        "                  np.where((res['lasttrip_time'] >= 1801) & (res['lasttrip_time'] <= 1900), 18,\n",
        "                  np.where((res['lasttrip_time'] >= 1901) & (res['lasttrip_time'] <= 2000), 19,\n",
        "                  np.where((res['lasttrip_time'] >= 2001) & (res['lasttrip_time'] <= 2100), 20,\n",
        "                  np.where((res['lasttrip_time'] >= 2101) & (res['lasttrip_time'] <= 2200), 21,\n",
        "                  np.where((res['lasttrip_time'] >= 2201) & (res['lasttrip_time'] <= 2300), 22,\n",
        "                  np.where((res['lasttrip_time'] >= 2301) & (res['lasttrip_time'] <= 2359), 23,-9))))))))))))))))))))))))\n",
        "x = res.groupby(['HOUSEID','PERSONID']).sum()# this has no effect and it is done to just prevent an error\n",
        "df_p = df_p.merge(x, how=\"left\", left_on=['HOUSEID', 'PERSONID'], right_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Removing outliers from the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VedoH3glZK9x"
      },
      "outputs": [],
      "source": [
        "# trips longer than 2 hours and longer than 30 miles are removed in order to prevent bias\n",
        "df = df[df['TRVL_MIN'] < 120]\n",
        "\n",
        "# so are the trips longer than 30 miles\n",
        "df = df[df['TRPMILES'] < 30]\n",
        "\n",
        "# and also the trips that have no trips after them for more than 12 hours(720 minutes)\n",
        "df = df[df['DWELTIME'] < 720]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "uVUhNB8DdXEX",
        "outputId": "31b8fa6a-daa7-49a6-b514-bb7e1f527dc9"
      },
      "outputs": [],
      "source": [
        "df_p.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dweltime, travel time, and travel distance of work and shop trips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8FctlM6N5Jui"
      },
      "outputs": [],
      "source": [
        "# This makes 2 columns of mean travel time for work and shop trips \n",
        "new_df1=df.pivot_table(index=['HOUSEID', 'PERSONID'],columns='WHYTO',values='TRVL_MIN')\n",
        "new_df1.fillna(0, inplace=True)\n",
        "\n",
        "df_p = df_p.merge(new_df1, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={2: 'work_traveltime', 3: 'shop_traveltime'}, inplace=True)\n",
        "df_p.drop([4], axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LyBnDrdx1YUL"
      },
      "outputs": [],
      "source": [
        "# Make a copy of distance column unchanged\n",
        "df['TRPMILES2'] = df['TRPMILES'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "v9dG8L35pQrV"
      },
      "outputs": [],
      "source": [
        "# Sum of distance traveled by the person\n",
        "new_df2=df.groupby(['HOUSEID','PERSONID'] )['TRPMILES'].sum()\n",
        "df_p = df_p.merge(new_df2, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'TRPMILES': 'TRPMILES_sum'}, inplace=True)\n",
        "\n",
        "# Sum of the travel time of the person\n",
        "new_df3=df.groupby(['HOUSEID','PERSONID'] )['TRVL_MIN'].sum()\n",
        "df_p = df_p.merge(new_df3, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'TRVL_MIN': 'TRVL_MIN_sum'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Normalizing trip distances due to the residence state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each state varies from others in terms of infrustructer development, urban or rural, state enormity, etc. thus the travel distance is normalized as follow to alleviate this problem and establish integrity in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Each trip is normalized by the max value of trip in that state\n",
        "df[\"max_trpmiles\"] = df.groupby(\"URBRUR\")[\"TRPMILES\"].transform(\"max\")\n",
        "df[\"TRPMILES\"] /= df[\"max_trpmiles\"]\n",
        "df = df.drop(\"max_trpmiles\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PQ18IdOdONIl"
      },
      "outputs": [],
      "source": [
        "new_df=df.pivot_table(index=['HOUSEID', 'PERSONID'],columns='WHYTO',values='TRPMILES')\n",
        "new_df.fillna(0, inplace=True)\n",
        "\n",
        "df_p = df_p.merge(new_df, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={2: 'work_tripmile', 3: 'shop_tripmile'}, inplace=True)\n",
        "df_p.drop([4], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GbO7ZDLhPbFx"
      },
      "outputs": [],
      "source": [
        "# trip miles for each person, normalized by state and urban-rural classification\n",
        "s=df.groupby(['HOUSEID',  'PERSONID'])['TRPMILES'].mean()\n",
        "df_p = df_p.merge(s, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'TRPMILES': 'TRPMILES_mean'}, inplace=True)\n",
        "\n",
        "#mean traveltime of all trips\n",
        "s1=df.groupby(['HOUSEID',  'PERSONID'])['TRVL_MIN'].mean()\n",
        "df_p = df_p.merge(s1, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'TRVL_MIN': 'TRVL_MIN_mean'}, inplace=True)\n",
        "\n",
        "#mean of the time the person is not traveling\n",
        "s2=df.groupby(['HOUSEID',  'PERSONID'])['DWELTIME'].mean()\n",
        "df_p = df_p.merge(s2, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'DWELTIME': 'DWELTIME_mean'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wRFcskgJOWWZ"
      },
      "outputs": [],
      "source": [
        "# Mean time spent for shopping and work \n",
        "new_df2=df.pivot_table(index=['HOUSEID', 'PERSONID'],columns='WHYTO',values='DWELTIME')\n",
        "new_df2.fillna(0, inplace=True)\n",
        "\n",
        "df_p = df_p.merge(new_df2, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={2: 'work_dweltime', 3: 'shop_dweltime'}, inplace=True)\n",
        "df_p.drop([4], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KDV3biHBrEqg"
      },
      "outputs": [],
      "source": [
        "# Counting the number of home based and non home based trips\n",
        "counts = df.groupby([\"HOUSEID\", \"PERSONID\", \"TRIPPURP\"]).apply(len).unstack(fill_value=0)\n",
        "counts.columns = counts.columns.map(lambda x: f\"{x}\")\n",
        "df_p = df_p.merge(counts, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "MSYf_Cql2nJ1"
      },
      "outputs": [],
      "source": [
        "# defining 3 columns that shows the number of trips by category\n",
        "counts5 = df.groupby([\"HOUSEID\", \"PERSONID\", \"TRPTRANS\"]).apply(len).unstack(fill_value=0)\n",
        "counts5.columns = counts5.columns.map(lambda x: f\"TRPTRANS{x}\")\n",
        "df_p = df_p.merge(counts5, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'TRPTRANS2.0': 'auto','TRPTRANS3.0': 'transit', 'TRPTRANS4.0': 'walk',\n",
        "                     'TRPTRANS5.0': 'bike', 'TRPTRANS6.0': 'other'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "So-C57Xv4VKU"
      },
      "outputs": [],
      "source": [
        "# This makes a colum with calculated speed for each trip in miles/hour.(distance/travel time)\n",
        "df['TRAVEL_SPEED'] = df['TRPMILES2']/df['TRVL_MIN']\n",
        "df['TRAVEL_SPEED'] *= 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7FC3ZEES6Gx7"
      },
      "outputs": [],
      "source": [
        "# make average speed for each mode\n",
        "new_df22=df.pivot_table(index=['HOUSEID', 'PERSONID'],columns='TRPTRANS',values='TRAVEL_SPEED')\n",
        "new_df22.fillna(0, inplace=True)\n",
        "\n",
        "df_p = df_p.merge(new_df22, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={2: 'auto_speed'}, inplace=True)\n",
        "df_p.drop([4, 3, 5, 6], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hP1w6U4WO-E4"
      },
      "outputs": [],
      "source": [
        "# setting under 18 people as np.nan\n",
        "\n",
        "list_under18 = [i for i in range(0,18)]\n",
        "\n",
        "df_p['R_AGE'].replace(list_under18, np.nan, inplace=True)\n",
        "\n",
        "df_p['lasttrip_time'].replace(-9, np.nan, inplace=True)\n",
        "df_p['firsttrip_time'].replace(-9, np.nan, inplace=True)\n",
        "\n",
        "df_p.dropna(axis=0, inplace=True)\n",
        "df_p.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "jczlICvRGISq",
        "outputId": "7bc9d8fa-acb1-4938-e887-0205897876ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(214211, 49)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_p.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JGMsph3py9gs"
      },
      "outputs": [],
      "source": [
        "df_p.to_csv('./data/processed_person_nhts_data.csv')\n",
        "nhts_df_p = df_p.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare and normalize the data for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Only the week days are kept for modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the models only on the samples collected with travelday == weekdays\n",
        "nhts_df_p = nhts_df_p[nhts_df_p['TRAVDAY'] == 'weekday']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A list of numerical features chosen for modeling\n",
        "cols_to_norm = ['WORK_EDUCATION',\t'SHOP',\t'OTHER'\t,'AM'\t,'PM'\t,'MIDDAY'\t,'NIGHT',\t'firsttrip_time',\n",
        "                'lasttrip_time'\t,\t'work_traveltime'\t,'shop_traveltime','DWELTIME_mean','work_tripmile',\n",
        "\t\t\t\t'shop_tripmile','TRPMILES_mean',\t'TRVL_MIN_mean',\t'work_dweltime',\n",
        "\t\t\t\t'shop_dweltime', 'HBO',\t'HBSHOP',\t'HBW',\t'NHB']\n",
        "\n",
        "# Normalizing numerical features\n",
        "nhts_df_p[cols_to_norm] = nhts_df_p[cols_to_norm].apply(lambda x: (x - x.mean()) / (x.max()-x.min()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare age classification input and output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "#defining 5 age ranges\n",
        "nhts_df_p['R_AGE_CAT'] = np.where((nhts_df_p['R_AGE'] >= 18) & (nhts_df_p['R_AGE'] <= 40), 1,\n",
        "                         np.where((nhts_df_p['R_AGE'] >= 41) & (nhts_df_p['R_AGE'] <= 50), 2,\n",
        "                         np.where((nhts_df_p['R_AGE'] >= 51) & (nhts_df_p['R_AGE'] <= 60), 3, 4)))\n",
        "\n",
        "X_nhts_age = nhts_df_p[cols_to_norm]\n",
        "y_nhts_age = nhts_df_p['R_AGE_CAT']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare income classification input and output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contracting income category to 5 classes\n",
        "nhts_df_p['HHFAMINC'].replace([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 1, inplace=True) # 0$-50000$ = 1\n",
        "nhts_df_p['HHFAMINC'].replace([11, 12, 13, 14, 15, 16, 17], 2, inplace=True) # 50000$-100000$ = 2\n",
        "nhts_df_p['HHFAMINC'].replace([18], 3, inplace=True) # 100000$ and more = 3\n",
        "\n",
        "# Copy the dataframe and only drops the nulls of income\n",
        "nhts_df_p_incom = nhts_df_p.copy()\n",
        "\n",
        "nhts_df_p_incom['HHFAMINC'].replace([-1,-9,-7,-8], np.nan, inplace=True)\n",
        "nhts_df_p_incom.dropna(axis=0, inplace=True)\n",
        "nhts_df_p_incom.reset_index(drop=True, inplace=True)\n",
        "\n",
        "X_nhts_income = nhts_df_p_incom[cols_to_norm]\n",
        "y_nhts_income = nhts_df_p_incom['HHFAMINC']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare gender classification input and output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eliminate people older than 60 for the gender prediction model\n",
        "nhts_df_p = nhts_df_p[nhts_df_p['R_AGE'] < 60]\n",
        "\n",
        "X_nhts_sex = nhts_df_p[cols_to_norm]\n",
        "y_nhts_sex = nhts_df_p['R_SEX']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2007/2008 TPB Household Travel Survey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df is Travel day trip dataframe\n",
        "df = pd.read_csv('./data/Washington household survey/hts07_TPB_tf.csv')\n",
        "\n",
        "# df_p is each person's dataframe\n",
        "df_p = pd.read_csv('./data/Washington household survey/hts07_TPB_pf.csv')\n",
        "\n",
        "#df_h is for household\n",
        "df_h = pd.read_csv('./data/Washington household survey/hts07_TPB_hf.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [],
      "source": [
        "# renaming columns like NHTS data\n",
        "df.rename(columns={'sampn': 'HOUSEID', 'personid': 'PERSONID','rtripid': 'TDTRPNUM','oact1': 'WHYFROM', 'rdact1': 'WHYTO',\n",
        "                   'tt': 'TRVL_MIN','dist': 'TRPMILES','begt': 'STRTTIME','endt': 'ENDTIME'}, inplace=True)\n",
        "df_p.rename(columns={'sampn': 'HOUSEID', 'personid': 'PERSONID','age': 'R_AGE',\n",
        "                     'gend': 'R_SEX'}, inplace=True)\n",
        "df_h.rename(columns={'sampn': 'HOUSEID', 'hhsiz': 'HHSIZE','hhveh': 'HHVEHCNT', 'incom':'HHFAMINC'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [],
      "source": [
        "# variables we wish to keep\n",
        "df = df[\n",
        "    ['HOUSEID', 'PERSONID','WHYFROM' , 'WHYTO',\n",
        "     'TRPMILES', 'STRTTIME','ENDTIME','TDTRPNUM',\n",
        "     'TRVL_MIN', 'pmode']\n",
        "]\n",
        "df_p = df_p[\n",
        "    ['HOUSEID', 'PERSONID', 'R_SEX',\n",
        "     'R_AGE', 'schol','wkstat', 'hours',\n",
        "     'relate']\n",
        "]\n",
        "df_h = df_h[\n",
        "    ['HOUSEID', 'HHFAMINC','HHSIZE',\n",
        "     'HHVEHCNT', 'home_tpb_newtaz', 'hhwrk']\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "# adding values from the household file to the person file\n",
        "df_p = df_p.join(df_h.set_index('HOUSEID'), on='HOUSEID')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### family income categories are coded as follow:   \n",
        "- 1 =Less than $10,000\n",
        "- 2 =$10,000 - $14,999\n",
        "\n",
        "- 3 =$15,000 - $29,999\n",
        "\n",
        "- 4 =$30,000 - $39,999\n",
        "- 5 =$40,000 - $49,999\n",
        "- 6 =$50,000 - $59,999\n",
        "\n",
        "- 7 =$60,000 - $74,999\n",
        "- 8 =$75,000 - $99,999\n",
        "\n",
        "- 9 =$100,000 - $124,999\n",
        "- 10 =$125,000 - $149,999\n",
        "- 11 =$150,000 - $199,999\n",
        "- 12 =$200,000 or more\n",
        "\n",
        "To be consistent with nhts categorization we defined earlier, let's change this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [],
      "source": [
        "# changing time to military format, like NHTS\n",
        "df['STRTTIME'] = df['STRTTIME'].str.replace(':', '')\n",
        "df['ENDTIME'] = df['ENDTIME'].str.replace(':', '')\n",
        "df[['STRTTIME','ENDTIME']] = df[['STRTTIME','ENDTIME']].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There isn't a columns in TPB data with dweltime values, so let's make one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [],
      "source": [
        "# making DWELTIME colum which show the time spent at the destination\n",
        "\n",
        "def get_minutes(s):\n",
        "    return s//100 * 60 + s % 100\n",
        "\n",
        "df['DWELTIME'] = (get_minutes(df.STRTTIME)\n",
        "                     .groupby([df.HOUSEID, df.PERSONID])\n",
        "                     .shift(-1)\n",
        "                     .sub(get_minutes(df.ENDTIME)).fillna(0)\n",
        "                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "regrouping purposes as they are:\n",
        "-    1 = SLEEP/ REST\n",
        "-   2 = EAT / PREPARE A MEAL AT HOME\n",
        "-    3 = EAT A MEAL AT WORK\n",
        "-    4 = EAT A MEAL OUTSIDE HOME OR WORK\n",
        "-    5 = CARE FOR CHILDREN\n",
        "-    6 = CHANGE MODE OF TRANSPORTATION\n",
        "-    7 = PICK UP / DROP OFF SOMEONE OR SOMETHING\n",
        "-    8 = LOOP TRIP\n",
        "-    9 = WORK (REGULAR PLACE)\n",
        "-   10 = WORK AT HOME OR TELECOMMUTE (FOR PAY)\n",
        "-   11 = WORK AT OTHER LOCATION\n",
        "-   12 = WORK-RELATED\n",
        "-   13 = EDUCATION/ SCHOOL-RELATED ACTIVITY\n",
        "-   14 = STUDY / DO HOMEWORK\n",
        "-   15 = CHILDCARE / PRESCHOOL\n",
        "-   16 = SHOP IN STORE\n",
        "-   17 = SHOP BY PHONE / INTERNET / TV\n",
        "-   18 = QUICK STOP / DRIVE THRU\n",
        "-   19 = PERSONAL BUSINESS AT ESTABLISHMENT\n",
        "-   20 = PERSONAL BUSINESS BY PHONE / INTERNET\n",
        "-   21 = VISIT/ SOCIALIZE\n",
        "-   22 = ENTERTAINMENT\n",
        "-   23 = RECREATION / EXERCISE\n",
        "-   24 = CIVIC OR RELIGIOUS ACTIVITY\n",
        "-   25 = MAIL PACKAGE OR LETTER OR OTHER POSTAL\n",
        "-   26 = OTHER HOUSEHOLD ACTIVITY\n",
        "-   97 = OTHER\n",
        "\n",
        "to what they should be: (like we grouped in NHTS preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [],
      "source": [
        "# change the items in the given list with 11 and\n",
        "home_trips = [1, 2, 10, 26, 5]\n",
        "work_education = [9, 11, 12, 13] # = 2\n",
        "shop_trips = [16] # = 3\n",
        "other_trips = [3, 4, 6, 7, 8,\n",
        "               14, 15, 17, 18, 19,\n",
        "               20, 21, 22, 23, 24, 25,\n",
        "               97] # = 4\n",
        "# add 1 later to the other category\n",
        "\n",
        "# regrouping purpose of destination classification\n",
        "df['WHYTO'].replace(home_trips, 1, inplace=True)\n",
        "df['WHYTO'].replace(other_trips, 3, inplace=True)\n",
        "df['WHYTO'].replace(work_education, 4, inplace=True)\n",
        "df['WHYTO'].replace(shop_trips, 2, inplace=True)\n",
        "\n",
        "# regrouping purpose of origin classification\n",
        "df['WHYFROM'].replace(home_trips, 1, inplace=True)\n",
        "df['WHYFROM'].replace(other_trips, 3, inplace=True)\n",
        "df['WHYFROM'].replace(work_education, 4, inplace=True)\n",
        "df['WHYFROM'].replace(shop_trips, 2, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [],
      "source": [
        "# making home-based and non-homebased column\n",
        "def hb_nhb(whyfrom, whyto):\n",
        "  if whyfrom != 1 and whyto != 1:\n",
        "    return 'NHB'\n",
        "  else:\n",
        "    if whyfrom == 4 or whyto == 4:\n",
        "      return 'HBW'\n",
        "    elif whyfrom == 2 or whyto == 2:\n",
        "      return 'HBSHOP'\n",
        "    elif whyfrom == 3 or whyto == 3:\n",
        "      return 'HBO'\n",
        "\n",
        "df['TRIPPURP'] = df.apply(lambda row:hb_nhb(row['WHYFROM'], row['WHYTO']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Changing home purpose to other\n",
        "df['WHYTO'].replace(1, 3, inplace=True)\n",
        "df['WHYFROM'].replace(1, 3, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [],
      "source": [
        "# changing gender classification categories to 0(male) and 1(female)\n",
        "df_p['R_SEX'].replace(1, 0, inplace=True)\n",
        "df_p['R_SEX'].replace(2, 1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [],
      "source": [
        "#defining 4 time of the day intervals\n",
        "# 1 = A.M\n",
        "# 2 = P.M\n",
        "# 3 = mid_day\n",
        "# 4 = night\n",
        "\n",
        "df['TRVL_DAY_TIME'] = np.where((df['STRTTIME'] >= 500) & (df['STRTTIME'] <= 1000), 1, \n",
        "                  np.where((df['STRTTIME'] >= 1001) & (df['STRTTIME'] <= 1530), 2,\n",
        "                  np.where((df['STRTTIME'] >= 1531) & (df['STRTTIME'] <= 2000), 3, 4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['TRPMILES'].replace([-9], np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [],
      "source": [
        "#counting the number of home based and non home based trips\n",
        "counts = df.groupby([\"HOUSEID\", \"PERSONID\", \"TRIPPURP\"]).apply(len).unstack(fill_value=0)\n",
        "counts.columns = counts.columns.map(lambda x: f\"{x}\")\n",
        "df_p = df_p.merge(counts, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {},
      "outputs": [],
      "source": [
        "# defining 3 columns that shows the number of trips by category\n",
        "counts = df.groupby([\"HOUSEID\", \"PERSONID\", \"WHYTO\"]).apply(len).unstack(fill_value=0)\n",
        "counts.columns = counts.columns.map(lambda x: f\"WHYTO{x}\")\n",
        "df_p = df_p.merge(counts, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'WHYTO2': 'WORK_EDUCATION', 'WHYTO3': 'SHOP', 'WHYTO4': 'OTHER'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [],
      "source": [
        "# making the columns for each daytime and merging it to the person dataframe\n",
        "counts1 = df.groupby([\"HOUSEID\", \"PERSONID\", \"TRVL_DAY_TIME\"]).apply(len).unstack(fill_value=0)\n",
        "counts1.columns = counts1.columns.map(lambda x: f\"STRTTIME{x}\")\n",
        "df_p = df_p.merge(counts1, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'STRTTIME1': 'AM', 'STRTTIME2': 'PM', 'STRTTIME3': 'MIDDAY', 'STRTTIME4': 'NIGHT'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [],
      "source": [
        "#this two new columns will show the time the person did their first and last trips, as is obvious in the columns' name\n",
        "res =(df.sort_values([\"HOUSEID\",\"PERSONID\"])\n",
        "    .groupby([\"HOUSEID\", \"PERSONID\"], as_index=False)\n",
        "    .agg({\"STRTTIME\":\"first\",\"ENDTIME\":\"last\"})\n",
        "    .rename(columns={\"STRTTIME\":\"firsttrip_time\",\"ENDTIME\":\"lasttrip_time\"}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this will convert the number to integer numbers in range of 0 to 23\n",
        "res['firsttrip_time'] = np.where((res['firsttrip_time'] >= 0) & (res['firsttrip_time'] <= 100), 0, \n",
        "                  np.where((res['firsttrip_time'] >= 101) & (res['firsttrip_time'] <= 200), 1,\n",
        "                  np.where((res['firsttrip_time'] >= 201) & (res['firsttrip_time'] <= 300), 2,\n",
        "                  np.where((res['firsttrip_time'] >= 301) & (res['firsttrip_time'] <= 400), 3,\n",
        "                  np.where((res['firsttrip_time'] >= 401) & (res['firsttrip_time'] <= 500), 4,\n",
        "                  np.where((res['firsttrip_time'] >= 501) & (res['firsttrip_time'] <= 600), 5,\n",
        "                  np.where((res['firsttrip_time'] >= 601) & (res['firsttrip_time'] <= 700), 6,\n",
        "                  np.where((res['firsttrip_time'] >= 701) & (res['firsttrip_time'] <= 800), 7,\n",
        "                  np.where((res['firsttrip_time'] >= 801) & (res['firsttrip_time'] <= 900), 8,\n",
        "                  np.where((res['firsttrip_time'] >= 901) & (res['firsttrip_time'] <= 1000), 9,\n",
        "                  np.where((res['firsttrip_time'] >= 1001) & (res['firsttrip_time'] <= 1100), 10,\n",
        "                  np.where((res['firsttrip_time'] >= 1101) & (res['firsttrip_time'] <= 1200), 11,\n",
        "                  np.where((res['firsttrip_time'] >= 1201) & (res['firsttrip_time'] <= 1300), 12, \n",
        "                  np.where((res['firsttrip_time'] >= 1301) & (res['firsttrip_time'] <= 1400), 13,\n",
        "                  np.where((res['firsttrip_time'] >= 1401) & (res['firsttrip_time'] <= 1500), 14,\n",
        "                  np.where((res['firsttrip_time'] >= 1501) & (res['firsttrip_time'] <= 1600), 15,\n",
        "                  np.where((res['firsttrip_time'] >= 1601) & (res['firsttrip_time'] <= 1700), 16,\n",
        "                  np.where((res['firsttrip_time'] >= 1701) & (res['firsttrip_time'] <= 1800), 17,\n",
        "                  np.where((res['firsttrip_time'] >= 1801) & (res['firsttrip_time'] <= 1900), 18,\n",
        "                  np.where((res['firsttrip_time'] >= 1901) & (res['firsttrip_time'] <= 2000), 19,\n",
        "                  np.where((res['firsttrip_time'] >= 2001) & (res['firsttrip_time'] <= 2100), 20,\n",
        "                  np.where((res['firsttrip_time'] >= 2101) & (res['firsttrip_time'] <= 2200), 21,\n",
        "                  np.where((res['firsttrip_time'] >= 2201) & (res['firsttrip_time'] <= 2300), 22,\n",
        "                  np.where((res['firsttrip_time'] >= 2301) & (res['firsttrip_time'] <= 2359), 23,-9))))))))))))))))))))))))\n",
        "\n",
        "res['lasttrip_time'] = np.where((res['lasttrip_time'] >= 0) & (res['lasttrip_time'] <= 100), 0, \n",
        "                  np.where((res['lasttrip_time'] >= 101) & (res['lasttrip_time'] <= 200), 1,\n",
        "                  np.where((res['lasttrip_time'] >= 201) & (res['lasttrip_time'] <= 300), 2,\n",
        "                  np.where((res['lasttrip_time'] >= 301) & (res['lasttrip_time'] <= 400), 3,\n",
        "                  np.where((res['lasttrip_time'] >= 401) & (res['lasttrip_time'] <= 500), 4,\n",
        "                  np.where((res['lasttrip_time'] >= 501) & (res['lasttrip_time'] <= 600), 5,\n",
        "                  np.where((res['lasttrip_time'] >= 601) & (res['lasttrip_time'] <= 700), 6,\n",
        "                  np.where((res['lasttrip_time'] >= 701) & (res['lasttrip_time'] <= 800), 7,\n",
        "                  np.where((res['lasttrip_time'] >= 801) & (res['lasttrip_time'] <= 900), 8,\n",
        "                  np.where((res['lasttrip_time'] >= 901) & (res['lasttrip_time'] <= 1000), 9,\n",
        "                  np.where((res['lasttrip_time'] >= 1001) & (res['lasttrip_time'] <= 1100), 10,\n",
        "                  np.where((res['lasttrip_time'] >= 1101) & (res['lasttrip_time'] <= 1200), 11,\n",
        "                  np.where((res['lasttrip_time'] >= 1201) & (res['lasttrip_time'] <= 1300), 12, \n",
        "                  np.where((res['lasttrip_time'] >= 1301) & (res['lasttrip_time'] <= 1400), 13,\n",
        "                  np.where((res['lasttrip_time'] >= 1401) & (res['lasttrip_time'] <= 1500), 14,\n",
        "                  np.where((res['lasttrip_time'] >= 1501) & (res['lasttrip_time'] <= 1600), 15,\n",
        "                  np.where((res['lasttrip_time'] >= 1601) & (res['lasttrip_time'] <= 1700), 16,\n",
        "                  np.where((res['lasttrip_time'] >= 1701) & (res['lasttrip_time'] <= 1800), 17,\n",
        "                  np.where((res['lasttrip_time'] >= 1801) & (res['lasttrip_time'] <= 1900), 18,\n",
        "                  np.where((res['lasttrip_time'] >= 1901) & (res['lasttrip_time'] <= 2000), 19,\n",
        "                  np.where((res['lasttrip_time'] >= 2001) & (res['lasttrip_time'] <= 2100), 20,\n",
        "                  np.where((res['lasttrip_time'] >= 2101) & (res['lasttrip_time'] <= 2200), 21,\n",
        "                  np.where((res['lasttrip_time'] >= 2201) & (res['lasttrip_time'] <= 2300), 22,\n",
        "                  np.where((res['lasttrip_time'] >= 2301) & (res['lasttrip_time'] <= 2359), 23,-9))))))))))))))))))))))))\n",
        "x = res.groupby(['HOUSEID','PERSONID']).sum()# to prevent an error\n",
        "df_p = df_p.merge(x, how=\"left\", left_on=['HOUSEID', 'PERSONID'], right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this makes 2 columns of mean travel time for work and shop trips \n",
        "new_df1=df.pivot_table(index=['HOUSEID', 'PERSONID'],columns='WHYTO',values='TRVL_MIN')\n",
        "new_df1.fillna(0, inplace=True)\n",
        "\n",
        "df_p = df_p.merge(new_df1, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={2: 'work_traveltime', 3:'shop_traveltime'}, inplace=True)\n",
        "df_p.drop([4], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [],
      "source": [
        "#sum of all lenght traveled by the person\n",
        "new_df2=df.groupby(['HOUSEID','PERSONID'] )['TRPMILES'].sum()\n",
        "df_p = df_p.merge(new_df2, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'TRPMILES': 'TRPMILES_sum'}, inplace=True)\n",
        "\n",
        "#sum of all the time the person was traveling\n",
        "new_df3=df.groupby(['HOUSEID','PERSONID'] )['TRVL_MIN'].sum()\n",
        "df_p = df_p.merge(new_df3, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'TRVL_MIN': 'TRVL_MIN_sum'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['TRPMILES'] /= df['TRPMILES'].max()\n",
        "\n",
        "new_df=df.pivot_table(index=['HOUSEID', 'PERSONID'],columns='WHYTO',values='TRPMILES')\n",
        "new_df.fillna(0, inplace=True)\n",
        "\n",
        "df_p = df_p.merge(new_df, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={2: 'work_tripmile', 3: 'shop_tripmile'}, inplace=True)\n",
        "df_p.drop([4], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [],
      "source": [
        "#trip miles for each person, normalized by state and urban-rural classification\n",
        "s=df.groupby(['HOUSEID',  'PERSONID'])['TRPMILES'].mean()\n",
        "df_p = df_p.merge(s, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'TRPMILES': 'TRPMILES_mean'}, inplace=True)\n",
        "\n",
        "#mean traveltime of all trips\n",
        "s1=df.groupby(['HOUSEID',  'PERSONID'])['TRVL_MIN'].mean()\n",
        "df_p = df_p.merge(s1, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'TRVL_MIN': 'TRVL_MIN_mean'}, inplace=True)\n",
        "\n",
        "#mean of the time the person is not traveling\n",
        "s2=df.groupby(['HOUSEID',  'PERSONID'])['DWELTIME'].mean()\n",
        "df_p = df_p.merge(s2, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={'DWELTIME': 'DWELTIME_mean'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mean time spent  for shopping and work \n",
        "new_df2=df.pivot_table(index=['HOUSEID', 'PERSONID'],columns='WHYTO',values='DWELTIME')\n",
        "new_df2.fillna(0, inplace=True)\n",
        "\n",
        "df_p = df_p.merge(new_df2, how=\"left\", left_on=[\"HOUSEID\", \"PERSONID\"], right_index=True)\n",
        "df_p.rename(columns={2: 'work_dweltime', 3:'shop_dweltime'}, inplace=True)\n",
        "df_p.drop([4], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detting under 18 people as np.nan\n",
        "list_x_under_18 = [i for i in range(0,18)]\n",
        "df_p['R_AGE'].replace(list_x_under_18, np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_p.dropna(axis=0, inplace=True)\n",
        "df_p.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [],
      "source": [
        "tpb_df_p = df_p.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare and normalize the data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {},
      "outputs": [],
      "source": [
        "# normalizing data\n",
        "cols_to_norm = ['WORK_EDUCATION',\t'SHOP',\t'OTHER'\t,'AM'\t,'PM'\t,'MIDDAY'\t,'NIGHT',\t'firsttrip_time',\n",
        "                'lasttrip_time'\t,\t'work_traveltime'\t,'shop_traveltime','DWELTIME_mean','work_tripmile',\n",
        "\t\t\t\t'shop_tripmile','TRPMILES_mean',\t'TRVL_MIN_mean',\t'work_dweltime',\n",
        "\t\t\t\t'shop_dweltime', 'HBO',\t'HBSHOP',\t'HBW',\t'NHB']\n",
        "                  \n",
        "tpb_df_p[cols_to_norm] = tpb_df_p[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max()-x.min())) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare age classification input and output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Age ranges:\n",
        "- 0-18 = 1\n",
        "- 19-32 = 2\n",
        "- 33-46 = 3\n",
        "- 47-60 = 4\n",
        "- 60 and more = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {},
      "outputs": [],
      "source": [
        "#defining 5 age ranges\n",
        "tpb_df_p['R_AGE_CAT'] = np.where((tpb_df_p['R_AGE'] >= 18) & (tpb_df_p['R_AGE'] <= 40), 1,\n",
        "                         np.where((tpb_df_p['R_AGE'] >= 41) & (tpb_df_p['R_AGE'] <= 50), 2,\n",
        "                         np.where((tpb_df_p['R_AGE'] >= 51) & (tpb_df_p['R_AGE'] <= 60), 3,                                                                 \n",
        "                                  4)))                            \n",
        "\n",
        "X_tpb_age = tpb_df_p[cols_to_norm]\n",
        "y_tpb_age = tpb_df_p['R_AGE_CAT']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare income classification input and output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [],
      "source": [
        "#contracting income category to 5 classes\n",
        "tpb_df_p['HHFAMINC'].replace([1, 2, 3, 4, 5], 1, inplace=True) # 0$-50000$ = 1\n",
        "tpb_df_p['HHFAMINC'].replace([6, 7, 8], 2, inplace=True) # 50000$-100000$ = 2\n",
        "tpb_df_p['HHFAMINC'].replace([9, 10, 11, 12], 3, inplace=True) # 100000$ and more  = 3\n",
        "\n",
        "X_tpb_income = tpb_df_p[cols_to_norm]\n",
        "y_tpb_income = tpb_df_p['HHFAMINC']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare gender classification input and output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [],
      "source": [
        "tpb_df_p = tpb_df_p[tpb_df_p['R_AGE'] < 60]\n",
        "X_tpb_sex = tpb_df_p[cols_to_norm]\n",
        "y_tpb_sex = tpb_df_p['R_SEX']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "def catboost_model(X_train, X_test, y_train, y_test,\n",
        "                    learning_rate=0.01,bagging_temperature=0.95,\n",
        "                    l2_leaf_reg=10, iterations=100000, \n",
        "                    early_stopping_rounds=1000, border_count=128,\n",
        "                    depth=6, random_strength=2.87, task='clasification'):\n",
        "                    \n",
        "    # Set parameters\n",
        "    PARAMS_CATBOOST = dict()\n",
        "    PARAMS_CATBOOST['learning_rate']= learning_rate\n",
        "    PARAMS_CATBOOST['bagging_temperature']= bagging_temperature\n",
        "    PARAMS_CATBOOST['l2_leaf_reg'] = l2_leaf_reg # lambda, default 3, S: 300\n",
        "    PARAMS_CATBOOST['iterations']= iterations\n",
        "    PARAMS_CATBOOST['early_stopping_rounds']=early_stopping_rounds\n",
        "    PARAMS_CATBOOST['border_count']= border_count\n",
        "    PARAMS_CATBOOST['depth']= depth\n",
        "    PARAMS_CATBOOST['random_strength']= random_strength\n",
        "    PARAMS_CATBOOST['use_best_model']= True\n",
        "    # PARAMS_CATBOOST_REGRESSOR['logging_level'] = 'Silent'\n",
        "\n",
        "    cat_features=[]\n",
        "    if task =='regression':\n",
        "        boost_model = CatBoostRegressor(**PARAMS_CATBOOST)\n",
        "    elif task == 'clasification':\n",
        "        boost_model = CatBoostClassifier(**PARAMS_CATBOOST)\n",
        "\n",
        "\n",
        "    train_dataset = Pool(data=X_train,\n",
        "                        label=y_train,\n",
        "                        cat_features=cat_features)\n",
        "    val_dataset = Pool(data=X_test,\n",
        "                        label=y_test,\n",
        "                        cat_features=cat_features)\n",
        "\n",
        "    boost_model.fit(train_dataset,use_best_model=True, eval_set=val_dataset)\n",
        "    \n",
        "    return boost_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nn_model(X_train, X_test, y_train, y_test,\n",
        "            hidden_act='relu', task='clasification',\n",
        "            epochs=100, batch_size=128, verbose=0):\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    loss_fn = keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    inputs = keras.Input(X_train.shape[1],)\n",
        "    x = layers.Dense(128, activation=hidden_act)(inputs)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(128, activation=hidden_act)(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(X_train.shape[1], activation=hidden_act)(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = layers.Add()([x, inputs])\n",
        "    x = layers.Dense(X_train.shape[1], activation=hidden_act)(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(hidden_act)(x)\n",
        "    x = layers.Add()([x, inputs])\n",
        "\n",
        "    if task == 'regression':\n",
        "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
        "        \n",
        "    elif task == 'clasification':\n",
        "        y_train = (pd.get_dummies(y_train)).values\n",
        "        y_test = (pd.get_dummies(y_test)).values\n",
        "        class_number = y_train.shape[1]\n",
        "        outputs = layers.Dense(class_number, activation='softmax')(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(loss=loss_fn, optimizer=optimizer, metrics=['acc'])\n",
        "        \n",
        "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), verbose=verbose)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, X_test, y_test, mode='normal', print_results=False, average='weighted'):\n",
        "    \n",
        "    y_pred = model.predict(X_test)\n",
        "    if mode == 'regression':\n",
        "        y_pred = np.array([1 if i > 0.5 else 0 for i in y_pred])\n",
        "    elif mode == 'onehot':\n",
        "        # convert onehot encoded output to integer\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred,average=average)\n",
        "    recall = recall_score(y_test, y_pred,average=average)\n",
        "    f1 = f1_score(y_test, y_pred,average=average)\n",
        "    confusion = confusion_matrix(y_test, y_pred)\n",
        "    cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    if print_results == True:\n",
        "        print('Accuracy:', accuracy)\n",
        "        print('Precision:', precision)\n",
        "        print('Recall:', recall)\n",
        "        print('F1 score:', f1)\n",
        "        print('Confusion matrix:', confusion)\n",
        "        print('Cohen kappa score', cohen_kappa)\n",
        "\n",
        "    return accuracy, precision, recall, f1, confusion, cohen_kappa\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "def general_ml_models(x_train, x_test, y_train, y_test,\n",
        "                     kernel_type=\"rbf\", max_iteration=10000,\n",
        "                        n_neig=10, algorithm_type=\"auto\", n_est=100, boot_strap=True,\n",
        "                        max_feat=\"sqrt\"):\n",
        "    # defime models\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neig, weights=\"distance\", algorithm=algorithm_type)\n",
        "    rf = RandomForestClassifier(n_estimators =n_est, bootstrap=boot_strap, max_features=max_feat)\n",
        "    svm = SVC(kernel=kernel_type, max_iter=max_iteration, gamma=\"auto\")\n",
        "    adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=n_est, learning_rate=0.1)\n",
        "    Gb = GradientBoostingClassifier(random_state=0)\n",
        "    dt = DecisionTreeClassifier()\n",
        "    lr = LogisticRegression()\n",
        "    lgbm = LGBMClassifier()\n",
        "\n",
        "    # fit models\n",
        "    knn.fit(x_train, y_train)\n",
        "    rf.fit(x_train, y_train)\n",
        "    svm.fit(x_train, y_train)\n",
        "    adaboost.fit(x_train, y_train)\n",
        "    Gb.fit(x_train, y_train)\n",
        "    dt.fit(x_train, y_train)\n",
        "    #lr.fit(x_train, y_train)\n",
        "    lgbm.fit(x_train, y_train)\n",
        "\n",
        "    # predict \n",
        "    accuracy, precision, recall, \\\n",
        "    f1, confusion, cohen_kappa = evaluate(knn, x_test, y_test)\n",
        "    print('The accuracy score for knn is:\\n', accuracy)\n",
        "    \n",
        "    accuracy, precision, recall, \\\n",
        "    f1, confusion, cohen_kappa = evaluate(rf, x_test, y_test)\n",
        "    print('The accuracy score for rf is:\\n', accuracy)\n",
        "    \n",
        "    accuracy, precision, recall, \\\n",
        "    f1, confusion, cohen_kappa = evaluate(svm, x_test, y_test)\n",
        "    print('The accuracy score for svm is:\\n', accuracy)\n",
        "\n",
        "    accuracy, precision, recall, \\\n",
        "    f1, confusion, cohen_kappa = evaluate(adaboost, x_test, y_test)\n",
        "    print('The accuracy score for adaboost is:\\n', accuracy)\n",
        "\n",
        "    accuracy, precision, recall, \\\n",
        "    f1, confusion, cohen_kappa = evaluate(Gb, x_test, y_test)\n",
        "    print('The accuracy score for gradient boost is:\\n', accuracy)\n",
        "\n",
        "    accuracy, precision, recall, \\\n",
        "    f1, confusion, cohen_kappa = evaluate(dt, x_test, y_test)\n",
        "    print('The accuracy score for decision tree is:\\n', accuracy)\n",
        "\n",
        "    '''accuracy_score, precision_score, recall_score, \\\n",
        "    f1_score, confusion_matrix, cohen_kappa_score = evaluate(lr, x_test, y_test)\n",
        "    print('The accuracy score for logistic regression is:\\n', accuracy_score)'''\n",
        "\n",
        "    accuracy, precision, recall, \\\n",
        "    f1, confusion, cohen_kappa = evaluate(lgbm, x_test, y_test)\n",
        "    print('The  accuracy score for lightgbm is:\\n', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Split into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# NHTS\n",
        "X_train_nhts_age, X_test_nhts_age, y_train_nhts_age, y_test_nhts_age =train_test_split(X_nhts_age, y_nhts_age, test_size=0.3, random_state=432)\n",
        "# TPB\n",
        "X_train_tpb_age, X_test_tpb_age, y_train_tpb_age, y_test_tpb_age =train_test_split(X_tpb_age, y_tpb_age, test_size=0.3, random_state=432)\n",
        "\n",
        "# NHTS\n",
        "X_train_nhts_income, X_test_nhts_income, y_train_nhts_income, y_test_nhts_income =train_test_split(X_nhts_income, y_nhts_income, test_size=0.3, random_state=432)\n",
        "# TPB\n",
        "X_train_tpb_income, X_test_tpb_income, y_train_tpb_income, y_test_tpb_income =train_test_split(X_tpb_income, y_tpb_income, test_size=0.3, random_state=432)\n",
        "\n",
        "# NHTS\n",
        "X_train_nhts_sex, X_test_nhts_sex, y_train_nhts_sex, y_test_nhts_sex =train_test_split(X_nhts_sex, y_nhts_sex, test_size=0.3, random_state=432)\n",
        "# TPB\n",
        "X_train_tpb_sex, X_test_tpb_sex, y_train_tpb_sex, y_test_tpb_sex =train_test_split(X_tpb_sex, y_tpb_sex, test_size=0.3, random_state=432)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#nueral network\n",
        "\n",
        "#nhts\n",
        "nn_nhts_age = nn_model(X_train_nhts_age, X_test_nhts_age, y_train_nhts_age, y_test_nhts_age, verbose=1)\n",
        "#tpb\n",
        "\n",
        "nn_tpb_age = nn_model(X_train_tpb_age, X_test_tpb_age, y_train_tpb_age, y_test_tpb_age, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Catboost\n",
        "\n",
        "#nhts\n",
        "cat_nhts_age = catboost_model(X_train_nhts_age, X_test_nhts_age, y_train_nhts_age, y_test_nhts_age)\n",
        "#tpb\n",
        "cat_tpb_age = catboost_model(X_train_tpb_age, X_test_tpb_age, y_train_tpb_age, y_test_tpb_age)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# other models\n",
        "other_tpb_age = general_ml_models(X_train_tpb_age, X_test_tpb_age, y_train_tpb_age, y_test_tpb_age)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Income"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#nueral network\n",
        "\n",
        "#nhts\n",
        "nn_nhts_income = nn_model(X_train_nhts_income, X_test_nhts_income, y_train_nhts_income, y_test_nhts_income, verbose=1)\n",
        "#tpb\n",
        "nn_tpb_income = nn_model(X_train_tpb_income, X_test_tpb_income, y_train_tpb_income, y_test_tpb_income, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Catboost\n",
        "\n",
        "#nhts\n",
        "cat_nhts_income = catboost_model(X_train_nhts_income, X_test_nhts_income, y_train_nhts_income, y_test_nhts_income)\n",
        "#tpb\n",
        "cat_tpb_income = catboost_model(X_train_tpb_income, X_test_tpb_income, y_train_tpb_income, y_test_tpb_income)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# other models\n",
        "other_tpb_income = general_ml_models(X_train_tpb_income, X_test_tpb_income, y_train_tpb_income, y_test_tpb_income)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#nueral network\n",
        "\n",
        "#nhts\n",
        "nn_nhts_gender = nn_model(X_train_nhts_sex, X_test_nhts_sex, y_train_nhts_sex, y_test_nhts_sex, task='regression', verbose=1)\n",
        "#tpb\n",
        "nn_tpb_gender = nn_model(X_train_tpb_sex, X_test_tpb_sex, y_train_tpb_sex, y_test_tpb_sex, task='regression', verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Catboost\n",
        "\n",
        "#nhts\n",
        "cat_nhts_gender = catboost_model(X_train_nhts_sex, X_test_nhts_sex, y_train_nhts_sex, y_test_nhts_sex, task='regression')\n",
        "#tpb\n",
        "cat_tpb_gender = catboost_model(X_train_tpb_sex, X_test_tpb_sex, y_train_tpb_sex, y_test_tpb_sex, task='regression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Other models\n",
        "other_tpb_gender = general_ml_models(X_train_tpb_sex, X_test_tpb_sex, y_train_tpb_sex, y_test_tpb_sex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# nn nhts\n",
        "evaluate(nn_nhts_age, X_test_nhts_age, y_test_nhts_age, mode='onehot', print_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cat nhts\n",
        "evaluate(cat_nhts_age, X_test_nhts_age, y_test_nhts_age, mode='normal', print_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# nn tpb\n",
        "evaluate(nn_tpb_age, X_test_tpb_age, y_test_tpb_age, mode='onehot', print_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cat tpb\n",
        "evaluate(cat_tpb_age, X_test_tpb_age, y_test_tpb_age, mode='normal', print_results=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Income"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# nn nhts\n",
        "evaluate(nn_nhts_income, X_test_nhts_income, y_test_nhts_income, mode='onehot', print_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cat nhts\n",
        "evaluate(cat_nhts_income, X_test_nhts_income, y_test_nhts_income, mode='normal', print_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# nn tpb\n",
        "evaluate(nn_tpb_income, X_test_tpb_income, y_test_tpb_income, mode='onehot', print_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cat tpb\n",
        "evaluate(cat_tpb_income, X_test_tpb_income, y_test_tpb_income, mode='normal', print_results=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# nn nhts\n",
        "evaluate(nn_nhts_gender, X_test_nhts_sex, y_test_nhts_sex, mode='regression', print_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cat nhts\n",
        "evaluate(cat_nhts_gender, X_test_nhts_sex, y_test_nhts_sex, mode='regression', print_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# nn tpb\n",
        "evaluate(nn_tpb_gender, X_test_tpb_sex, y_test_tpb_sex, mode='regression', print_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cat tpb\n",
        "evaluate(cat_tpb_gender,X_test_tpb_sex, y_test_tpb_sex, mode='regression', print_results=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Post modeling -> Bayesian inference for income"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section synthesized data is used to improve the probalities of income and age predictions based on Bayesian updating "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "syn_df = pd.read_csv('./data/New_Households_base_year2.csv.flattened.csv',index_col=0)\n",
        "\n",
        "syn_df.shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "syn_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        " #setting under 18 people as np.nan\n",
        "syn_df['AGE'].replace(list_under18, np.nan, inplace=True)\n",
        "syn_df.dropna(axis=0, inplace=True)\n",
        "syn_df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "#defining 5 age ranges\n",
        "syn_df['INCOME'] = np.where((syn_df['INCOME'] >= 0) & (syn_df['INCOME'] <= 50000), 1,\n",
        "                         np.where((syn_df['INCOME'] >= 50001) & (syn_df['INCOME'] <= 100000), 2,                \n",
        "                            3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "#defining 5 age ranges\n",
        "syn_df['AGE'] = np.where((syn_df['AGE'] >= 18) & (syn_df['AGE'] <= 40), 1,\n",
        "                         np.where((syn_df['AGE'] >= 41) & (syn_df['AGE'] <= 50), 2,\n",
        "                         np.where((syn_df['AGE'] >= 51) & (syn_df['AGE'] <= 60), 3,                 \n",
        "                            4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "# changing gender classification categories to 0(male) and 1(female)\n",
        "syn_df['GENDER'].replace(1, 0, inplace=True)\n",
        "syn_df['GENDER'].replace(2, 1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#grouby counts and calculate the prevalent distribution percentage for each location\n",
        "counts_income = syn_df.groupby([\"LOCATION\", \"INCOME\"]).apply(len).unstack(fill_value=0)\n",
        "counts_income['sum'] = counts_income[1]+counts_income[2]+counts_income[3]\n",
        "counts_income[1] = counts_income[1]/counts_income['sum']\n",
        "counts_income[2] = counts_income[2]/counts_income['sum']\n",
        "counts_income[3] = counts_income[3]/counts_income['sum']\n",
        "del counts_income['sum']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#grouby counts and calculate the prevalent distribution percentage for each location\n",
        "counts_age = syn_df.groupby([\"LOCATION\", \"AGE\"]).apply(len).unstack(fill_value=0)\n",
        "counts_age['sum'] = counts_age[1]+counts_age[2]+counts_age[3]+counts_age[4]\n",
        "counts_age[1] = counts_age[1]/counts_age['sum']\n",
        "counts_age[2] = counts_age[2]/counts_age['sum']\n",
        "counts_age[3] = counts_age[3]/counts_age['sum']\n",
        "counts_age[4] = counts_age[4]/counts_age['sum']\n",
        "del counts_age['sum']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#grouby counts and calculate the prevalent distribution percentage for each location\n",
        "counts_sex = syn_df.groupby([\"LOCATION\", \"GENDER\"]).apply(len).unstack(fill_value=0)\n",
        "counts_sex['sum'] = counts_sex[0]+counts_sex[1]\n",
        "counts_sex[0] = counts_sex[0]/counts_sex['sum']\n",
        "counts_sex[1] = counts_sex[1]/counts_sex['sum']\n",
        "del counts_sex['sum']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [],
      "source": [
        "#making possiblity dictionaries\n",
        "income_likelihood_dict = counts_income.T.to_dict('list')\n",
        "age_likelihood_dict = counts_age.T.to_dict('list')\n",
        "sex_likelihood_dict = counts_sex.T.to_dict('list')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now predict the income probabilities from the model and update them based on TAZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict labels for all the data\n",
        "y_pred_tpb_income = cat_tpb_income.predict(X_tpb_income)\n",
        "\n",
        "# predict probabilies for all the data\n",
        "y_pred_proba_tpb_income = cat_tpb_income.predict_proba(X_tpb_income)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert numpy array to dataframe \n",
        "df = pd.DataFrame(y_pred_proba_tpb_income, columns =['prob1', 'prob2', 'prob3'])\n",
        "\n",
        "#assign location to the probablity dataframe\n",
        "df['Location'] = tpb_df_p['home_tpb_newtaz']\n",
        "\n",
        "#assign y_true to the dataframe\n",
        "df['y_true'] = y_tpb_income\n",
        "\n",
        "#predicted y\n",
        "df['y_pred'] = y_pred_tpb_income"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [],
      "source": [
        "#formula bays. update probability\n",
        "def bays_update(prob1, prob2, prob3, location):\n",
        "    if location in income_likelihood_dict:\n",
        "        list_prob = income_likelihood_dict[location]\n",
        "        liky_1 = list_prob[0]# likelyhood = liky\n",
        "        liky_2 = list_prob[1]\n",
        "        liky_3 = list_prob[2]\n",
        "        #soorat kasr\n",
        "        numerator1 = (liky_1*prob1) \n",
        "        numerator2 = (liky_2*prob2)\n",
        "        numerator3 = (liky_3*prob3)\n",
        "        #makhraj kasr\n",
        "        evidence = numerator1+numerator2+numerator3\n",
        "        posterior1 = numerator1/evidence\n",
        "        posterior2 = numerator2/evidence\n",
        "        posterior3 = numerator3/evidence\n",
        "        return pd.Series([posterior1,posterior2,posterior3])\n",
        "    else:\n",
        "        return pd.Series([prob1,prob2,prob3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['updated_prob1'] = ''\n",
        "df['updated_prob2'] = ''\n",
        "df['updated_prob3'] = ''\n",
        "df[['updated_prob1', 'updated_prob2', 'updated_prob3']] = (df.apply(lambda row:bays_update(row['prob1'], row['prob2'], row['prob3'], row['Location']),axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [],
      "source": [
        "def max_value(prob1,prob2,prob3):\n",
        "  maximum = max(prob1,prob2,prob3)\n",
        "  if maximum == prob1:\n",
        "    return 1\n",
        "  elif maximum == prob2:\n",
        "    return 2\n",
        "  elif maximum == prob3:\n",
        "    return 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['updated_pred'] = (df.apply(lambda row:max_value(row['updated_prob1'], row['updated_prob2'], row['updated_prob3']),axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_score(df['y_true'], df['updated_pred'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# both data are prepared in the same manner so these experiments are possible to conduct\n",
        "X_nhts_age.columns == X_tpb_age.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1- Train on NHTS and validate on TPB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ex_age_model = nn_model(X_nhts_age, X_tpb_age, y_nhts_age, y_tpb_age, epochs=10, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ex_income_model = nn_model(X_nhts_income, X_tpb_income, y_nhts_income, y_tpb_income, epochs=10, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ex_gender_model = nn_model(X_nhts_sex, X_tpb_sex, y_nhts_sex, y_tpb_sex, task='regression', epochs=10, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2- Use the NHTS model to improve TPB model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# predict probabilities\n",
        "\n",
        "y_tpb_on_nhts_age = cat_nhts_age.predict_proba(X_tpb_age)\n",
        "y_tpb_on_nhts_income = cat_nhts_income.predict_proba(X_tpb_income)\n",
        "y_tpb_on_nhts_gender = cat_nhts_gender.predict(X_tpb_sex)\n",
        "\n",
        "# change the numpy array to dataframe\n",
        "y_tpb_on_nhts_age = pd.DataFrame(y_tpb_on_nhts_age)\n",
        "y_tpb_on_nhts_income = pd.DataFrame(y_tpb_on_nhts_income)\n",
        "y_tpb_on_nhts_gender = pd.DataFrame(y_tpb_on_nhts_gender)\n",
        "\n",
        "# reset index and contacenate\n",
        "X_tpb_age.reset_index(drop=True, inplace=True)\n",
        "X_tpb_income.reset_index(drop=True, inplace=True)\n",
        "X_tpb_sex.reset_index(drop=True, inplace=True)\n",
        "\n",
        "X_tpb_age = pd.concat([X_tpb_age, y_tpb_on_nhts_age], axis=1)\n",
        "X_tpb_income = pd.concat([X_tpb_income, y_tpb_on_nhts_income], axis=1)\n",
        "X_tpb_sex = pd.concat([X_tpb_sex, y_tpb_on_nhts_gender], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split the new data for training\n",
        "\n",
        "X_train_tpb_age, X_test_tpb_age, y_train_tpb_age, y_test_tpb_age = train_test_split(X_tpb_age, y_tpb_age, test_size=0.3, random_state=432)\n",
        "X_train_tpb_income, X_test_tpb_income, y_train_tpb_income, y_test_tpb_income = train_test_split(X_tpb_income, y_tpb_income, test_size=0.3, random_state=432)\n",
        "X_train_tpb_sex, X_test_tpb_sex, y_train_tpb_sex, y_test_tpb_sex = train_test_split(X_tpb_sex, y_tpb_sex, test_size=0.3, random_state=432)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_tpb_age_exp = catboost_model(X_train_tpb_age, X_test_tpb_age, y_train_tpb_age, y_test_tpb_age)\n",
        "evaluate(cat_tpb_age_exp, X_test_tpb_age, y_test_tpb_age, print_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_tpb_income_exp = catboost_model(X_train_tpb_income, X_test_tpb_income, y_train_tpb_income, y_test_tpb_income)\n",
        "evaluate(cat_tpb_income_exp, X_test_tpb_income, y_test_tpb_income, print_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_tpb_gender_exp = catboost_model(X_train_tpb_sex, X_test_tpb_sex, y_train_tpb_sex, y_test_tpb_sex, task='regression')\n",
        "evaluate(cat_tpb_gender_exp, X_test_tpb_sex, y_test_tpb_sex, mode='regression', print_results=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NSTH_Dataframe_maker.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
